{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755d75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with: python nhanes_wearable_diabetes.py\n",
    "# required: pandas, numpy, scikit-learn, wget (or run wget from shell)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    average_precision_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Import PhysioNet package (one-time)\n",
    "# -----------------------------\n",
    "# If you want to download via command-line from the PhysioNet host, run:\n",
    "# wget -r -N -c -np https://physionet.org/files/minute-level-step-count-nhanes/1.0.1/\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Paths (update if you put files elsewhere)\n",
    "# -----------------------------\n",
    "base_dir = \"./data/nhanes-step-count/\"   # set to where you put the PhysioNet folder\n",
    "subject_info_path = os.path.join(base_dir, \"subject-info.csv\")\n",
    "actisteps_path = os.path.join(base_dir, \"nhanes_1440_actisteps.csv.xz\")\n",
    "ac_path = os.path.join(base_dir, \"nhanes_1440_AC.csv.xz\")          # activity counts (if present)\n",
    "mims_path = os.path.join(base_dir, \"nhanes_1440_PAXMTSM.csv.xz\")  # MIMS summary (if present)\n",
    "\n",
    "# sanity check files exist\n",
    "for p in [subject_info_path, actisteps_path]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\\nPlease download the PhysioNet package and set base_dir correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd48cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject-info.csv ...\n",
      "subject-info rows: 19931\n",
      "['SEQN', 'data_release_cycle', 'gender', 'age_in_years_at_screening', 'full_sample_2_year_interview_weight', 'full_sample_2_year_mec_exam_weight', 'masked_variance_pseudo_psu', 'masked_variance_pseudo_stratum']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 2. Load subject info (demographics)\n",
    "# -----------------------------\n",
    "print(\"Loading subject-info.csv ...\")\n",
    "subj = pd.read_csv(subject_info_path, dtype={\"SEQN\": str})\n",
    "# keep SEQN as string for consistent merging\n",
    "subj[\"SEQN\"] = subj[\"SEQN\"].astype(str)\n",
    "print(\"subject-info rows:\", len(subj))\n",
    "print(subj.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7a5dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading daily actisteps (compressed)... This may take a few minutes.\n",
      "Found minute columns: 1440\n",
      "Per-subject aggregated step features: (14693, 7)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Load daily actisteps file and compute per-day totals -> per-subject features\n",
    "# -----------------------------\n",
    "# The actisteps file contains one row per subject-day with columns: SEQN, PAXDAYM, PAXDAYWM, min_0001 ... min_1440\n",
    "# We'll compute daily_sum = sum(min_0001...min_1440), mark valid day (daily_sum > 0), then aggregate per SEQN.\n",
    "print(\"Loading daily actisteps (compressed)... This may take a few minutes.\")\n",
    "df_actisteps = pd.read_csv(actisteps_path, dtype={\"SEQN\": str}, low_memory=False)\n",
    "# Identify minute columns (min_0001 ... min_1440)\n",
    "minute_cols = [c for c in df_actisteps.columns if c.startswith(\"min_\")]\n",
    "print(\"Found minute columns:\", len(minute_cols))\n",
    "\n",
    "# compute daily totals (sum across minutes)\n",
    "df_actisteps[\"daily_steps\"] = df_actisteps[minute_cols].sum(axis=1, numeric_only=True)\n",
    "\n",
    "# create a 'valid_day' mask: require daily_steps > 0 and not flagged as missing/quality flag if available\n",
    "# (PAXFLGSM or similar flags may exist in other files; for simplicity use daily_steps>0)\n",
    "df_actisteps[\"valid_day\"] = df_actisteps[\"daily_steps\"] > 0\n",
    "\n",
    "# aggregate per subject\n",
    "agg = df_actisteps.groupby(\"SEQN\").agg(\n",
    "    mean_daily_steps=(\"daily_steps\", \"mean\"),\n",
    "    median_daily_steps=(\"daily_steps\", \"median\"),\n",
    "    sd_daily_steps=(\"daily_steps\", \"std\"),\n",
    "    valid_day_count=(\"valid_day\", \"sum\"),\n",
    "    total_days=(\"daily_steps\", \"size\")\n",
    ").reset_index()\n",
    "\n",
    "# if sd is NaN (single day), fill with 0\n",
    "agg[\"sd_daily_steps\"] = agg[\"sd_daily_steps\"].fillna(0.0)\n",
    "agg[\"pct_valid_days\"] = agg[\"valid_day_count\"] / agg[\"total_days\"]\n",
    "\n",
    "print(\"Per-subject aggregated step features:\", agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2efa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AC (activity counts) file to compute mean AC per day...\n",
      "Loading MIMS file to get mean MIMS per day...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Optional: load AC / MIMS files (if you want activity counts / MIMS)\n",
    "# -----------------------------\n",
    "if os.path.exists(ac_path):\n",
    "    print(\"Loading AC (activity counts) file to compute mean AC per day...\")\n",
    "    df_ac = pd.read_csv(ac_path, dtype={\"SEQN\": str}, low_memory=False)\n",
    "    # AC file rows are daily with min_ columns again\n",
    "    min_cols_ac = [c for c in df_ac.columns if c.startswith(\"min_\")]\n",
    "    df_ac[\"daily_AC_sum\"] = df_ac[min_cols_ac].sum(axis=1, numeric_only=True)\n",
    "    ac_agg = df_ac.groupby(\"SEQN\").agg(\n",
    "        mean_daily_AC=(\"daily_AC_sum\", \"mean\"),\n",
    "        sd_daily_AC=(\"daily_AC_sum\", \"std\")\n",
    "    ).reset_index()\n",
    "    ac_agg[\"sd_daily_AC\"] = ac_agg[\"sd_daily_AC\"].fillna(0.0)\n",
    "    # merge into agg\n",
    "    agg = agg.merge(ac_agg, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "if os.path.exists(mims_path):\n",
    "    print(\"Loading MIMS file to get mean MIMS per day...\")\n",
    "    df_mims = pd.read_csv(mims_path, dtype={\"SEQN\": str}, low_memory=False)\n",
    "    # PAXMTSM daily summary (min_ cols are MIMS per minute)\n",
    "    min_cols_mims = [c for c in df_mims.columns if c.startswith(\"min_\")]\n",
    "    df_mims[\"daily_mims_sum\"] = df_mims[min_cols_mims].sum(axis=1, numeric_only=True)\n",
    "    mims_agg = df_mims.groupby(\"SEQN\").agg(\n",
    "        mean_daily_mims=(\"daily_mims_sum\", \"mean\")\n",
    "    ).reset_index()\n",
    "    agg = agg.merge(mims_agg, on=\"SEQN\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218d18ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging aggregated wearable features with subject-info...\n",
      "Merged shape: (14693, 17)\n",
      "Sample columns: ['SEQN', 'mean_daily_steps', 'median_daily_steps', 'sd_daily_steps', 'valid_day_count', 'total_days', 'pct_valid_days', 'mean_daily_AC', 'sd_daily_AC', 'mean_daily_mims', 'data_release_cycle', 'gender', 'age_in_years_at_screening', 'full_sample_2_year_interview_weight', 'full_sample_2_year_mec_exam_weight', 'masked_variance_pseudo_psu', 'masked_variance_pseudo_stratum']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Merge with subject-info to bring in demographics (Age, Sex, BMI if present)\n",
    "# -----------------------------\n",
    "# subject-info columns differ; common ones include AGE, Sex, etc. Inspect and pick.\n",
    "print(\"Merging aggregated wearable features with subject-info...\")\n",
    "df = agg.merge(subj, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# rename some demographic columns if needed (inspect)\n",
    "print(\"Merged shape:\", df.shape)\n",
    "print(\"Sample columns:\", df.columns.tolist()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002b3895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NHANES glycohemoglobin (HbA1c) files from CDC...\n",
      "Columns in GHB file: ['SEQN', 'LBXGH_G', 'LBXGH_H']\n",
      "Glycohemoglobin rows after concat: 12788\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Load NHANES Glycohemoglobin files (2011-2012 and 2013-2014) and merge LBXGH\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Reading NHANES glycohemoglobin (HbA1c) files from CDC...\")\n",
    "try:\n",
    "    ghb_g = pd.read_sas(\"./data/nhanes-lab/ghb-2011-12.xpt\", format=\"xport\")\n",
    "    ghb_h = pd.read_sas(\"./data/nhanes-lab/ghb-2013-14.xpt\", format=\"xport\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Could not read GHB XPT files via pandas.read_sas. \"\n",
    "                       \"You can manually download the XPTs from the CDC NHANES pages and point ghb_g_path to local file.\") from e\n",
    "\n",
    "# standardize LBXGH column name (NHANES uses LBXGH for HbA1c %)\n",
    "ghb_g = ghb_g[[\"SEQN\", \"LBXGH\"]].rename(columns={\"LBXGH\": \"LBXGH_G\"})\n",
    "ghb_h = ghb_h[[\"SEQN\", \"LBXGH\"]].rename(columns={\"LBXGH\": \"LBXGH_H\"})\n",
    "\n",
    "# combine rows: prefer G (2011-2012) then H (2013-2014) depending on SEQN (SEQN is unique per cycle),\n",
    "# but simplest is concatenate then dedupe latest if duplicated.\n",
    "ghb = pd.concat([ghb_g, ghb_h], ignore_index=True)\n",
    "print(\"Columns in GHB file:\", ghb.columns.tolist())\n",
    "ghb[\"HBA1C\"] = ghb[\"LBXGH_G\"].fillna(ghb[\"LBXGH_H\"])\n",
    "ghb = ghb.dropna(subset=[\"HBA1C\"])\n",
    "ghb = ghb.drop_duplicates(subset=[\"SEQN\"], keep=\"first\")\n",
    "\n",
    "print(\"Glycohemoglobin rows after concat:\", len(ghb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a1523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging with HbA1c, shape: (11303, 20)\n",
      "Final dataset for modeling: n = 11303\n",
      "Class balance:\n",
      " diabetes_binary\n",
      "0    10244\n",
      "1     1059\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Merge wearable features with glycohemoglobin (on SEQN) to make modeling table\n",
    "# -----------------------------\n",
    "df[\"SEQN\"] = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ghb[\"SEQN\"] = pd.to_numeric(ghb[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df.merge(ghb, on=\"SEQN\", how=\"inner\")\n",
    "print(\"After merging with HbA1c, shape:\", df.shape)\n",
    "\n",
    "# create diabetes label (0/1). Option: include prediabetes as 1 if you want\n",
    "# Standard cutoffs:\n",
    "#   normal < 5.7\n",
    "#   prediabetes 5.7-6.4\n",
    "#   diabetes >= 6.5\n",
    "df[\"diabetes_binary\"] = (df[\"HBA1C\"] >= 6.5).astype(int)\n",
    "# if LBXGH_G/H column names differ, adjust above logic\n",
    "\n",
    "print(\"Final dataset for modeling: n =\", len(df))\n",
    "print(\"Class balance:\\n\", df[\"diabetes_binary\"].value_counts())\n",
    "\n",
    "# create gender label (0: male, 1: female)\n",
    "df[\"gender\"] = df[\"gender\"].map({\"Male\": 0, \"Female\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee3f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'mean_daily_steps', 'median_daily_steps', 'sd_daily_steps', 'valid_day_count', 'total_days', 'pct_valid_days', 'mean_daily_AC', 'sd_daily_AC', 'mean_daily_mims', 'data_release_cycle', 'gender', 'age_in_years_at_screening', 'full_sample_2_year_interview_weight', 'full_sample_2_year_mec_exam_weight', 'masked_variance_pseudo_psu', 'masked_variance_pseudo_stratum', 'LBXGH_G', 'LBXGH_H', 'HBA1C', 'diabetes_binary']\n",
      "Using features: ['mean_daily_steps', 'median_daily_steps', 'sd_daily_steps', 'pct_valid_days', 'mean_daily_AC', 'mean_daily_mims', 'gender', 'age_in_years_at_screening', 'full_sample_2_year_mec_exam_weight']\n",
      "Split sizes (train/val/test): 6781 2261 2261\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 8. Select model features & train/test split\n",
    "# -----------------------------\n",
    "# Choose features from aggregated step features + some demographics if present\n",
    "# Try: mean_daily_steps, sd_daily_steps, pct_valid_days, Age, BMI, RIAGENDR (gender), RIDAGEYR (age)\n",
    "# Check for column existence and create X accordingly:\n",
    "print(df.columns.tolist())\n",
    "feat_candidates = [\n",
    "    \"mean_daily_steps\", \"median_daily_steps\", \"sd_daily_steps\", \"pct_valid_days\",\n",
    "    # optional AC/MIMS\n",
    "    \"mean_daily_AC\", \"mean_daily_mims\",\n",
    "    # demographics (column names vary in subject-info.csv; check common ones)\n",
    "    \"gender\", \"age_in_years_at_screening\", \"full_sample_2_year_mec_exam_weight\"\n",
    "]\n",
    "# pick present features\n",
    "features = [f for f in feat_candidates if f in df.columns]\n",
    "print(\"Using features:\", features)\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[\"diabetes_binary\"].astype(int)\n",
    "\n",
    "# simple fillna (median) for any leftover missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# stratified splits\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.40, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Split sizes (train/val/test):\", len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba26b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation threshold: 0.18115577889447237 F1: 0.27722772277227725\n",
      "Adjusted (recall-boosted) threshold: 0.1449246231155779\n",
      "Test Precision: 0.21761658031088082\n",
      "Test Recall: 0.5943396226415094\n",
      "Test F1: 0.3185840707964602\n",
      "Test PR AUC: 0.22100175445963807\n",
      "Confusion Matrix:\n",
      " [[1596  453]\n",
      " [  86  126]]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Train Random Forest and thresholding as before\n",
    "# -----------------------------\n",
    "model = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# choose threshold by maximizing F1 on validation\n",
    "y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "best_t, best_f1 = 0.0, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 200):\n",
    "    preds = (y_val_prob >= t).astype(int)\n",
    "    score = f1_score(y_val, preds)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_t = t\n",
    "print(\"Best validation threshold:\", best_t, \"F1:\", best_f1)\n",
    "\n",
    "# lower threshold to boost recall\n",
    "adjusted_t = best_t * 0.8\n",
    "print(\"Adjusted (recall-boosted) threshold:\", adjusted_t)\n",
    "\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_prob >= adjusted_t).astype(int)\n",
    "\n",
    "print(\"Test Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Test PR AUC:\", average_precision_score(y_test, y_test_prob))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diapredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
