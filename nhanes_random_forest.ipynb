{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755d75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Setup Data Paths\n",
    "nhanes_step_count_dir = \"./data/nhanes-step-count/\"\n",
    "subject_info_path = os.path.join(nhanes_step_count_dir, \"subject-info.csv\")\n",
    "actisteps_path = os.path.join(nhanes_step_count_dir, \"nhanes_1440_actisteps.csv.xz\")\n",
    "ac_path = os.path.join(nhanes_step_count_dir, \"nhanes_1440_AC.csv.xz\")\n",
    "mims_path = os.path.join(nhanes_step_count_dir, \"nhanes_1440_PAXMTSM.csv.xz\")\n",
    "\n",
    "nhanes_lab_dir = \"./data/nhanes-lab/\"\n",
    "ghb_path_2011 = os.path.join(nhanes_lab_dir, \"ghb-2011-12.xpt\")\n",
    "ghb_path_2013 = os.path.join(nhanes_lab_dir, \"ghb-2013-14.xpt\")\n",
    "\n",
    "nhanes_questionnaire_dir = \"./data/nhanes-questionnaire/\"\n",
    "bpq_path_2011 = os.path.join(nhanes_questionnaire_dir, \"bpq-2011-12.xpt\")\n",
    "bpq_path_2013 = os.path.join(nhanes_questionnaire_dir, \"bpq-2013-14.xpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd48cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wearable Movement Data... (this may take a few minutes)\n",
      "Wearable Data Shape:\n",
      "- Subject Info: (19931, 8)\n",
      "- Actisteps: (130186, 1443)\n",
      "- Activity Counts: (130186, 1443)\n",
      "- MIMS: (130186, 1443)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 2. Load NHANES Step-Count Data\n",
    "print(\"Loading Wearable Movement Data... (this may take a few minutes)\")\n",
    "\n",
    "subj_df = pd.read_csv(subject_info_path, dtype={\"SEQN\": \"Int64\"})\n",
    "actisteps_df = pd.read_csv(actisteps_path, dtype={\"SEQN\": \"Int64\"}, low_memory=False)\n",
    "ac_df = pd.read_csv(ac_path, dtype={\"SEQN\": \"Int64\"}, low_memory=False)\n",
    "mims_df = pd.read_csv(mims_path, dtype={\"SEQN\": \"Int64\"}, low_memory=False)\n",
    "\n",
    "print(\"Wearable Data Shape:\")\n",
    "print(\"- Subject Info:\", subj_df.shape)\n",
    "print(\"- Actisteps:\", actisteps_df.shape)\n",
    "print(\"- Activity Counts:\", ac_df.shape)\n",
    "print(\"- MIMS:\", mims_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7a5dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Wearable Data Features...\n",
      "['SEQN', 'data_release_cycle', 'gender', 'age_in_years_at_screening', 'full_sample_2_year_interview_weight', 'full_sample_2_year_mec_exam_weight', 'masked_variance_pseudo_psu', 'masked_variance_pseudo_stratum']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>mean_daily_steps</th>\n",
       "      <th>sd_daily_steps</th>\n",
       "      <th>mean_daily_AC</th>\n",
       "      <th>sd_daily_AC</th>\n",
       "      <th>mean_daily_mims</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14685.0</td>\n",
       "      <td>14685.000000</td>\n",
       "      <td>14685.000000</td>\n",
       "      <td>1.468500e+04</td>\n",
       "      <td>1.468500e+04</td>\n",
       "      <td>14685.000000</td>\n",
       "      <td>14685.000000</td>\n",
       "      <td>14685.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73183.777528</td>\n",
       "      <td>9696.561159</td>\n",
       "      <td>4827.944617</td>\n",
       "      <td>2.151068e+06</td>\n",
       "      <td>1.063271e+06</td>\n",
       "      <td>11352.391422</td>\n",
       "      <td>0.510861</td>\n",
       "      <td>35.753899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6486.391406</td>\n",
       "      <td>3703.772504</td>\n",
       "      <td>1902.143937</td>\n",
       "      <td>8.833311e+05</td>\n",
       "      <td>4.435279e+05</td>\n",
       "      <td>4329.524632</td>\n",
       "      <td>0.499899</td>\n",
       "      <td>23.184655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>62161.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.256192e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.344667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>67313.0</td>\n",
       "      <td>7313.111111</td>\n",
       "      <td>3593.250270</td>\n",
       "      <td>1.571732e+06</td>\n",
       "      <td>7.575077e+05</td>\n",
       "      <td>8623.339000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>74099.0</td>\n",
       "      <td>9805.111111</td>\n",
       "      <td>4756.055824</td>\n",
       "      <td>2.129310e+06</td>\n",
       "      <td>1.028943e+06</td>\n",
       "      <td>11335.417778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78971.0</td>\n",
       "      <td>12158.666667</td>\n",
       "      <td>5954.645665</td>\n",
       "      <td>2.734121e+06</td>\n",
       "      <td>1.333755e+06</td>\n",
       "      <td>14213.382000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83731.0</td>\n",
       "      <td>29042.222222</td>\n",
       "      <td>18750.860952</td>\n",
       "      <td>7.206275e+06</td>\n",
       "      <td>7.410255e+06</td>\n",
       "      <td>35693.532444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEQN  mean_daily_steps  sd_daily_steps  mean_daily_AC  \\\n",
       "count       14685.0      14685.000000    14685.000000   1.468500e+04   \n",
       "mean   73183.777528       9696.561159     4827.944617   2.151068e+06   \n",
       "std     6486.391406       3703.772504     1902.143937   8.833311e+05   \n",
       "min         62161.0          1.500000        0.000000   1.256192e+02   \n",
       "25%         67313.0       7313.111111     3593.250270   1.571732e+06   \n",
       "50%         74099.0       9805.111111     4756.055824   2.129310e+06   \n",
       "75%         78971.0      12158.666667     5954.645665   2.734121e+06   \n",
       "max         83731.0      29042.222222    18750.860952   7.206275e+06   \n",
       "\n",
       "        sd_daily_AC  mean_daily_mims        gender           age  \n",
       "count  1.468500e+04     14685.000000  14685.000000  14685.000000  \n",
       "mean   1.063271e+06     11352.391422      0.510861     35.753899  \n",
       "std    4.435279e+05      4329.524632      0.499899     23.184655  \n",
       "min    0.000000e+00         1.344667      0.000000      3.000000  \n",
       "25%    7.575077e+05      8623.339000      0.000000     14.000000  \n",
       "50%    1.028943e+06     11335.417778      1.000000     33.000000  \n",
       "75%    1.333755e+06     14213.382000      1.000000     55.000000  \n",
       "max    7.410255e+06     35693.532444      1.000000     80.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Feature Engineering for NHANES Step-Count Data\n",
    "print(\"Computing Wearable Data Features...\")\n",
    "\n",
    "# Compute Daily Step Statistics\n",
    "actisteps_minute_cols = [c for c in actisteps_df.columns if c.startswith(\"min_\")]\n",
    "actisteps_df[\"daily_steps\"] = actisteps_df[actisteps_minute_cols].sum(axis=1, numeric_only=True)\n",
    "actisteps_df[\"valid_day\"] = actisteps_df[\"daily_steps\"] > 0\n",
    "actisteps_df = actisteps_df[actisteps_df[\"valid_day\"]]\n",
    "actisteps_agg = actisteps_df.groupby(\"SEQN\").agg(\n",
    "    mean_daily_steps=(\"daily_steps\", \"mean\"),\n",
    "    sd_daily_steps=(\"daily_steps\", \"std\"),\n",
    ").reset_index()\n",
    "actisteps_agg[\"sd_daily_steps\"] = actisteps_agg[\"sd_daily_steps\"].fillna(0.0)\n",
    "\n",
    "selected_columns = [\"SEQN\", \"mean_daily_steps\", \"sd_daily_steps\"]\n",
    "df = actisteps_agg[selected_columns].copy()\n",
    "\n",
    "# Compute Activity Counts Features\n",
    "ac_minute_cols = [c for c in ac_df.columns if c.startswith(\"min_\")]\n",
    "ac_df[\"daily_AC\"] = ac_df[ac_minute_cols].sum(axis=1, numeric_only=True)\n",
    "ac_agg = ac_df.groupby(\"SEQN\").agg(\n",
    "    mean_daily_AC=(\"daily_AC\", \"mean\"),\n",
    "    sd_daily_AC=(\"daily_AC\", \"std\")\n",
    ").reset_index()\n",
    "ac_agg[\"sd_daily_AC\"] = ac_agg[\"sd_daily_AC\"].fillna(0.0)\n",
    "\n",
    "selected_columns = [\"SEQN\", \"mean_daily_AC\", \"sd_daily_AC\"]\n",
    "ac_agg = ac_agg[selected_columns].copy()\n",
    "df = df.merge(ac_agg, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# Compute MIMS (Monitor-Independent Movement Summary) Features\n",
    "mims_minute_cols = [c for c in mims_df.columns if c.startswith(\"min_\")]\n",
    "mims_df[\"daily_mims_sum\"] = mims_df[mims_minute_cols].sum(axis=1, numeric_only=True)\n",
    "mims_agg = mims_df.groupby(\"SEQN\").agg(\n",
    "    mean_daily_mims=(\"daily_mims_sum\", \"mean\"),\n",
    ").reset_index()\n",
    "selected_columns = [\"SEQN\", \"mean_daily_mims\"]\n",
    "mims_agg = mims_agg[selected_columns].copy()\n",
    "df = df.merge(mims_agg, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# Merge with Subject Info\n",
    "print(subj_df.columns.tolist())\n",
    "selected_columns = [\"SEQN\", \"gender\", \"age_in_years_at_screening\"]\n",
    "subj_df = subj_df[selected_columns].copy()\n",
    "subj_df['gender'] = subj_df['gender'].map({'Male': 0, 'Female': 1})\n",
    "subj_df = subj_df.rename(columns={'age_in_years_at_screening': 'age'})\n",
    "df = df.merge(subj_df, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce0c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>mean_daily_steps</th>\n",
       "      <th>sd_daily_steps</th>\n",
       "      <th>mean_daily_AC</th>\n",
       "      <th>sd_daily_AC</th>\n",
       "      <th>mean_daily_mims</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>BPQ020</th>\n",
       "      <th>BPQ080</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10089.0</td>\n",
       "      <td>10089.000000</td>\n",
       "      <td>10089.000000</td>\n",
       "      <td>1.008900e+04</td>\n",
       "      <td>1.008900e+04</td>\n",
       "      <td>10089.000000</td>\n",
       "      <td>10089.000000</td>\n",
       "      <td>10089.000000</td>\n",
       "      <td>10089.000000</td>\n",
       "      <td>10089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73319.499455</td>\n",
       "      <td>9298.063513</td>\n",
       "      <td>4623.720766</td>\n",
       "      <td>1.955659e+06</td>\n",
       "      <td>9.580977e+05</td>\n",
       "      <td>10368.249539</td>\n",
       "      <td>0.520765</td>\n",
       "      <td>47.337595</td>\n",
       "      <td>1.652295</td>\n",
       "      <td>1.715135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6476.847021</td>\n",
       "      <td>3719.139896</td>\n",
       "      <td>1989.634256</td>\n",
       "      <td>7.710494e+05</td>\n",
       "      <td>4.011098e+05</td>\n",
       "      <td>3712.342248</td>\n",
       "      <td>0.499593</td>\n",
       "      <td>18.598677</td>\n",
       "      <td>0.526108</td>\n",
       "      <td>0.743949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>62161.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.256192e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.344667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>67473.0</td>\n",
       "      <td>6844.111111</td>\n",
       "      <td>3286.032905</td>\n",
       "      <td>1.470669e+06</td>\n",
       "      <td>6.875406e+05</td>\n",
       "      <td>8112.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>74259.0</td>\n",
       "      <td>9184.444444</td>\n",
       "      <td>4441.714368</td>\n",
       "      <td>1.945696e+06</td>\n",
       "      <td>9.247005e+05</td>\n",
       "      <td>10423.461333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79132.0</td>\n",
       "      <td>11603.444444</td>\n",
       "      <td>5735.964430</td>\n",
       "      <td>2.448192e+06</td>\n",
       "      <td>1.188918e+06</td>\n",
       "      <td>12794.944111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83729.0</td>\n",
       "      <td>26553.222222</td>\n",
       "      <td>18750.860952</td>\n",
       "      <td>6.911811e+06</td>\n",
       "      <td>5.098803e+06</td>\n",
       "      <td>29617.966444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEQN  mean_daily_steps  sd_daily_steps  mean_daily_AC  \\\n",
       "count       10089.0      10089.000000    10089.000000   1.008900e+04   \n",
       "mean   73319.499455       9298.063513     4623.720766   1.955659e+06   \n",
       "std     6476.847021       3719.139896     1989.634256   7.710494e+05   \n",
       "min         62161.0          1.500000        0.000000   1.256192e+02   \n",
       "25%         67473.0       6844.111111     3286.032905   1.470669e+06   \n",
       "50%         74259.0       9184.444444     4441.714368   1.945696e+06   \n",
       "75%         79132.0      11603.444444     5735.964430   2.448192e+06   \n",
       "max         83729.0      26553.222222    18750.860952   6.911811e+06   \n",
       "\n",
       "        sd_daily_AC  mean_daily_mims        gender           age  \\\n",
       "count  1.008900e+04     10089.000000  10089.000000  10089.000000   \n",
       "mean   9.580977e+05     10368.249539      0.520765     47.337595   \n",
       "std    4.011098e+05      3712.342248      0.499593     18.598677   \n",
       "min    0.000000e+00         1.344667      0.000000     16.000000   \n",
       "25%    6.875406e+05      8112.730000      0.000000     31.000000   \n",
       "50%    9.247005e+05     10423.461333      1.000000     47.000000   \n",
       "75%    1.188918e+06     12794.944111      1.000000     62.000000   \n",
       "max    5.098803e+06     29617.966444      1.000000     80.000000   \n",
       "\n",
       "             BPQ020        BPQ080  \n",
       "count  10089.000000  10089.000000  \n",
       "mean       1.652295      1.715135  \n",
       "std        0.526108      0.743949  \n",
       "min        1.000000      1.000000  \n",
       "25%        1.000000      1.000000  \n",
       "50%        2.000000      2.000000  \n",
       "75%        2.000000      2.000000  \n",
       "max        9.000000      9.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 - Load NHANES Blood Pressure Questionnaire Data + Merge Features\n",
    "\n",
    "bpq_2011 = pd.read_sas(bpq_path_2011, format=\"xport\")\n",
    "bpq_2013 = pd.read_sas(bpq_path_2013, format=\"xport\")\n",
    "\n",
    "bpq = pd.concat([bpq_2011, bpq_2013], ignore_index=True)\n",
    "# BPQ020: Ever told you had high blood pressure\n",
    "# BPQ080: Doctor told you - high cholesterol level\n",
    "selected_columns = [\"SEQN\", \"BPQ020\", \"BPQ080\"]\n",
    "bpq = bpq[selected_columns].copy()\n",
    "bpq = bpq.dropna()\n",
    "df = df.merge(bpq, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002b3895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lab A1C Data...\n",
      "Lab Data Shape:\n",
      "- GHB 2011-2012: (6549, 2)\n",
      "- GHB 2013-2014: (6979, 2)\n",
      "Calculating ground truth with A1C lab data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>mean_daily_steps</th>\n",
       "      <th>sd_daily_steps</th>\n",
       "      <th>mean_daily_AC</th>\n",
       "      <th>sd_daily_AC</th>\n",
       "      <th>mean_daily_mims</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>BPQ020</th>\n",
       "      <th>BPQ080</th>\n",
       "      <th>diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9693.0</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9.693000e+03</td>\n",
       "      <td>9.693000e+03</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9693.000000</td>\n",
       "      <td>9693.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73345.937687</td>\n",
       "      <td>9341.622032</td>\n",
       "      <td>4638.183336</td>\n",
       "      <td>1.965061e+06</td>\n",
       "      <td>9.610760e+05</td>\n",
       "      <td>10415.181679</td>\n",
       "      <td>0.520891</td>\n",
       "      <td>47.399360</td>\n",
       "      <td>1.652017</td>\n",
       "      <td>1.712679</td>\n",
       "      <td>0.375116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6474.360186</td>\n",
       "      <td>3695.534337</td>\n",
       "      <td>1983.649429</td>\n",
       "      <td>7.659998e+05</td>\n",
       "      <td>3.994481e+05</td>\n",
       "      <td>3683.158220</td>\n",
       "      <td>0.499589</td>\n",
       "      <td>18.507968</td>\n",
       "      <td>0.528125</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.484178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>62161.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.862902e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.344667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>67492.0</td>\n",
       "      <td>6893.333333</td>\n",
       "      <td>3305.466193</td>\n",
       "      <td>1.481845e+06</td>\n",
       "      <td>6.913610e+05</td>\n",
       "      <td>8173.797111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>74300.0</td>\n",
       "      <td>9222.125000</td>\n",
       "      <td>4454.423105</td>\n",
       "      <td>1.952477e+06</td>\n",
       "      <td>9.265108e+05</td>\n",
       "      <td>10459.773333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79152.0</td>\n",
       "      <td>11621.111111</td>\n",
       "      <td>5741.104310</td>\n",
       "      <td>2.451345e+06</td>\n",
       "      <td>1.189246e+06</td>\n",
       "      <td>12816.167556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83729.0</td>\n",
       "      <td>26553.222222</td>\n",
       "      <td>18750.860952</td>\n",
       "      <td>6.911811e+06</td>\n",
       "      <td>5.098803e+06</td>\n",
       "      <td>29617.966444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEQN  mean_daily_steps  sd_daily_steps  mean_daily_AC  \\\n",
       "count        9693.0       9693.000000     9693.000000   9.693000e+03   \n",
       "mean   73345.937687       9341.622032     4638.183336   1.965061e+06   \n",
       "std     6474.360186       3695.534337     1983.649429   7.659998e+05   \n",
       "min         62161.0          3.000000        0.000000   1.862902e+02   \n",
       "25%         67492.0       6893.333333     3305.466193   1.481845e+06   \n",
       "50%         74300.0       9222.125000     4454.423105   1.952477e+06   \n",
       "75%         79152.0      11621.111111     5741.104310   2.451345e+06   \n",
       "max         83729.0      26553.222222    18750.860952   6.911811e+06   \n",
       "\n",
       "        sd_daily_AC  mean_daily_mims       gender          age       BPQ020  \\\n",
       "count  9.693000e+03      9693.000000  9693.000000  9693.000000  9693.000000   \n",
       "mean   9.610760e+05     10415.181679     0.520891    47.399360     1.652017   \n",
       "std    3.994481e+05      3683.158220     0.499589    18.507968     0.528125   \n",
       "min    0.000000e+00         1.344667     0.000000    16.000000     1.000000   \n",
       "25%    6.913610e+05      8173.797111     0.000000    32.000000     1.000000   \n",
       "50%    9.265108e+05     10459.773333     1.000000    47.000000     2.000000   \n",
       "75%    1.189246e+06     12816.167556     1.000000    62.000000     2.000000   \n",
       "max    5.098803e+06     29617.966444     1.000000    80.000000     9.000000   \n",
       "\n",
       "            BPQ080  diabetes_binary  \n",
       "count  9693.000000      9693.000000  \n",
       "mean      1.712679         0.375116  \n",
       "std       0.742608         0.484178  \n",
       "min       1.000000         0.000000  \n",
       "25%       1.000000         0.000000  \n",
       "50%       2.000000         0.000000  \n",
       "75%       2.000000         1.000000  \n",
       "max       9.000000         1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 Load NHANES Laboratory Glycohemoglobin Data + Calculate Ground Truth Diabetes Binary\n",
    "print(\"Loading Lab A1C Data...\")\n",
    "\n",
    "ghb_2011 = pd.read_sas(\"./data/nhanes-lab/ghb-2011-12.xpt\", format=\"xport\")\n",
    "ghb_2013 = pd.read_sas(\"./data/nhanes-lab/ghb-2013-14.xpt\", format=\"xport\")\n",
    "\n",
    "print(\"Lab Data Shape:\")\n",
    "print(\"- GHB 2011-2012:\", ghb_2011.shape)\n",
    "print(\"- GHB 2013-2014:\", ghb_2013.shape)\n",
    "\n",
    "print(\"Calculating ground truth with A1C lab data...\")\n",
    "ghb_2011 = ghb_2011[[\"SEQN\", \"LBXGH\"]].rename(columns={\"LBXGH\": \"a1c_2011\"})\n",
    "ghb_2013 = ghb_2013[[\"SEQN\", \"LBXGH\"]].rename(columns={\"LBXGH\": \"a1c_2013\"})\n",
    "\n",
    "# Combine 2011-12 and 2013-14 data\n",
    "ghb = pd.concat([ghb_2011, ghb_2013], ignore_index=True)\n",
    "ghb[\"a1c\"] = ghb[\"a1c_2011\"].combine_first(ghb[\"a1c_2013\"])\n",
    "ghb = ghb.dropna(subset=[\"a1c\"])\n",
    "ghb = ghb.drop_duplicates(subset=[\"SEQN\"], keep=\"first\")\n",
    "\n",
    "# A1C Diabetes Criteria (We include prediabetes as diabetes):\n",
    "#   normal < 5.7\n",
    "#   prediabetes 5.7-6.4\n",
    "#   diabetes >= 6.5\n",
    "ghb[\"diabetes_binary\"] = (ghb[\"a1c\"] >= 5.7).astype(int)\n",
    "\n",
    "selected_columns = [\"SEQN\", \"diabetes_binary\"]\n",
    "ghb = ghb[selected_columns].copy()\n",
    "df = df.merge(ghb, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee3f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training for train/val/test...\n",
      "Split sizes (train/val/test): 6785 1454 1454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 6 Split Data into Train/Val/Test:\n",
    "print(\"Splitting training for train/val/test...\")\n",
    "\n",
    "X = df.drop(columns=[\"SEQN\", \"diabetes_binary\"])\n",
    "y = df[\"diabetes_binary\"].astype(int)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Split sizes (train/val/test):\", len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba26b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 450 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 450 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     15\u001b[39m rf = RandomForestClassifier(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     16\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     17\u001b[39m     estimator=rf,\n\u001b[32m     18\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters found:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(grid_search.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnnev\\Desktop\\Git Repos and Coding\\CS3402\\DiaPredict\\diapredict\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# 6.5 - (Optional) Hyperparameter Tuning for Random Forest with GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(f1_score),\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use best estimator to predict on validation and test sets\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Validation F1:\", f1_score(y_val, y_val_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "\n",
    "# Best parameters found:\n",
    "# {'class_weight': 'balanced', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 900}\n",
    "# Best F1 score: 0.6544\n",
    "# Validation F1: 0.6537867078825348\n",
    "# Test F1: 0.6506211180124224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation threshold: 0.5339195979899497 F1: 0.6634304207119741\n",
      "Adjusted (recall-boosted) threshold: 0.4538316582914572\n",
      "Test Precision: 0.5460048426150121\n",
      "Test Recall: 0.8275229357798165\n",
      "Test F1: 0.6579139314369074\n",
      "Test PR AUC: 0.6005394568258001\n",
      "Confusion Matrix:\n",
      " [[534 375]\n",
      " [ 94 451]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    average_precision_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# 7. Train Random Forest Model\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=900, class_weight=\"balanced\", random_state=42, max_depth=5, min_samples_leaf=4, min_samples_split=2)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# choose threshold by maximizing F1 on validation\n",
    "y_val_prob = rf_model.predict_proba(X_val)[:, 1]\n",
    "best_t, best_f1 = 0.0, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 200):\n",
    "    preds = (y_val_prob >= t).astype(int)\n",
    "    score = f1_score(y_val, preds)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_t = t\n",
    "print(\"Best validation threshold:\", best_t, \"F1:\", best_f1)\n",
    "\n",
    "y_test_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_prob >= best_t).astype(int)\n",
    "\n",
    "print(\"Test Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Test PR AUC:\", average_precision_score(y_test, y_test_prob))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e3cac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300, 'scale_pos_weight': 2}\n",
      "Best F1 score: 0.6574\n",
      "Validation F1: 0.6568483063328424\n",
      "Test F1: 0.6563649742457689\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 7.5 - (Optional) Hyperparameter Tuning for XGBoost with GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'scale_pos_weight': [1, 2, 5]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring='f1', cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use best estimator to predict on validation and test sets\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Validation F1:\", f1_score(y_val, y_val_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd97a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Best validation threshold: 0.5791457286432161 F1: 0.6639871382636656\n",
      "\n",
      "--- XGBoost Results ---\n",
      "Test Precision: 0.5738636363636364\n",
      "Test Recall: 0.7412844036697248\n",
      "Test F1: 0.6469175340272217\n",
      "Test PR AUC: 0.6106223888114422\n",
      "Confusion Matrix:\n",
      " [[609 300]\n",
      " [141 404]]\n"
     ]
    }
   ],
   "source": [
    "# 8. Train XGBoost Model\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.01,\n",
    "    scale_pos_weight=2,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Threshold tuning on validation set\n",
    "y_val_prob_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "best_t_xgb, best_f1_xgb = 0.0, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 200):\n",
    "    preds = (y_val_prob_xgb >= t).astype(int)\n",
    "    score = f1_score(y_val, preds)\n",
    "    if score > best_f1_xgb:\n",
    "        best_f1_xgb = score\n",
    "        best_t_xgb = t\n",
    "print(\"XGBoost - Best validation threshold:\", best_t_xgb, \"F1:\", best_f1_xgb)\n",
    "\n",
    "y_test_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_xgb = (y_test_prob_xgb >= best_t_xgb).astype(int)\n",
    "\n",
    "print(\"\\n--- XGBoost Results ---\")\n",
    "print(\"Test Precision:\", precision_score(y_test, y_test_pred_xgb))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_test_pred_xgb))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred_xgb))\n",
    "print(\"Test PR AUC:\", average_precision_score(y_test, y_test_prob_xgb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea763511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (128, 64), 'learning_rate': 'constant', 'learning_rate_init': 0.01}\n",
      "Best CV F1 score: 0.6374\n",
      "\n",
      "Validation F1: 0.6332\n",
      "Test F1: 0.6088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 8.5 - (Optional) Hyperparameter Tuning for MLP Neural Network with GridSearchCV\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64,), (64, 32), (64, 32, 16), (128, 64), (128, 64, 32)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Use early_stopping in the base estimator to prevent overfitting\n",
    "mlp = MLPClassifier(\n",
    "    random_state=42,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best CV F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val_scaled)\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\nValidation F1: {f1_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Test F1: {f1_score(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d52e67b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Best validation threshold: 0.33492462311557786 F1: 0.6592095451155854\n",
      "\n",
      "--- Neural Network (MLP) Results ---\n",
      "Test Precision: 0.546583850931677\n",
      "Test Recall: 0.8073394495412844\n",
      "Test F1: 0.6518518518518519\n",
      "Test PR AUC: 0.5939069176997727\n",
      "Confusion Matrix:\n",
      " [[544 365]\n",
      " [105 440]]\n"
     ]
    }
   ],
   "source": [
    "# 9. Train Neural Network (MLP)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural networks benefit from feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights for the loss function\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    alpha=0.0001,\n",
    ")\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Threshold tuning on validation set\n",
    "y_val_prob_mlp = mlp_model.predict_proba(X_val_scaled)[:, 1]\n",
    "best_t_mlp, best_f1_mlp = 0.0, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 200):\n",
    "    preds = (y_val_prob_mlp >= t).astype(int)\n",
    "    score = f1_score(y_val, preds)\n",
    "    if score > best_f1_mlp:\n",
    "        best_f1_mlp = score\n",
    "        best_t_mlp = t\n",
    "print(\"MLP - Best validation threshold:\", best_t_mlp, \"F1:\", best_f1_mlp)\n",
    "\n",
    "y_test_prob_mlp = mlp_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred_mlp = (y_test_prob_mlp >= best_t_mlp).astype(int)\n",
    "\n",
    "print(\"\\n--- Neural Network (MLP) Results ---\")\n",
    "print(\"Test Precision:\", precision_score(y_test, y_test_pred_mlp))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_test_pred_mlp))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred_mlp))\n",
    "print(\"Test PR AUC:\", average_precision_score(y_test, y_test_prob_mlp))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71001402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights - RF: 0.332, XGB: 0.336, MLP: 0.332\n",
      "\n",
      "--- Weighted Average ---\n",
      "Threshold: 0.199\n",
      "Precision: 0.4617\n",
      "Recall: 0.9633\n",
      "F1: 0.6243\n",
      "PR AUC: 0.6090\n",
      "Confusion Matrix:\n",
      "[[297 612]\n",
      " [ 20 525]]\n"
     ]
    }
   ],
   "source": [
    "# 10. Ensemble Methods - Combining RF, XGBoost, and MLP\n",
    "\n",
    "# Get probability predictions from all models\n",
    "probs_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "probs_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "probs_mlp = mlp_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Get validation probabilities for threshold tuning\n",
    "val_probs_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "val_probs_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "val_probs_mlp = mlp_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Calculate Weights for Weighted Average based on validation PR-AUC\n",
    "w_rf = average_precision_score(y_val, val_probs_rf)\n",
    "w_xgb = average_precision_score(y_val, val_probs_xgb)\n",
    "w_mlp = average_precision_score(y_val, val_probs_mlp)\n",
    "total_w = w_rf + w_xgb + w_mlp\n",
    "w_rf, w_xgb, w_mlp = w_rf/total_w, w_xgb/total_w, w_mlp/total_w\n",
    "print(f\"Learned weights - RF: {w_rf:.3f}, XGB: {w_xgb:.3f}, MLP: {w_mlp:.3f}\")\n",
    "\n",
    "ensemble_weighted_val = w_rf * val_probs_rf + w_xgb * val_probs_xgb + w_mlp * val_probs_mlp\n",
    "ensemble_weighted_test = w_rf * probs_rf + w_xgb * probs_xgb + w_mlp * probs_mlp\n",
    "\n",
    "# Function to find optimal threshold for target recall\n",
    "def find_threshold_for_recall(y_true, y_prob, target_recall=0.95):\n",
    "    best_t, best_f1 = 0.0, -1.0\n",
    "    for t in np.linspace(0.01, 0.95, 300):\n",
    "        preds = (y_prob >= t).astype(int)\n",
    "        rec = recall_score(y_true, preds)\n",
    "        if rec >= target_recall:\n",
    "            f1 = f1_score(y_true, preds)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = t\n",
    "    return best_t, best_f1\n",
    "\n",
    "\n",
    "# Find threshold that achieves ~95% recall\n",
    "results = []\n",
    "best_t = 0.2 # Fallback\n",
    "best_t, _ = find_threshold_for_recall(y_val, ensemble_weighted_val, target_recall=0.95)\n",
    "\n",
    "\n",
    "y_pred = (ensemble_weighted_test >= best_t).astype(int)\n",
    "\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "pr_auc = average_precision_score(y_test, ensemble_weighted_test)\n",
    "\n",
    "print(f\"\\n--- Weighted Average ---\")\n",
    "print(f\"Threshold: {best_t:.3f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diapredict (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
